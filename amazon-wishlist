#!/usr/bin/python -tt

# amazon-wishlist v0.2
# Copyright (c) 2008, John Morrissey <jwm@horde.net>
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are
# met:
#
#	* Redistributions of source code must retain the above copyright notice,
#	  this list of conditions and the following disclaimer.
#	* Redistributions in binary form must reproduce the above copyright
#	  notice, this list of conditions and the following disclaimer in the
#	  documentation and/or other materials provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
# IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
# NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

AWS_ACCESS_KEY = ''
AWS_SECRET_KEY = ''
WISHLIST_ID = ''
AWS_XML_NS = 'http://webservices.amazon.com/AWSECommerceService/2005-10-05'

# MCLS branches I prefer.
BRANCHES = [
	'CENTRL', 'MGRW-E', 'EVNS-W', 'HENRI',
]


from base64 import b64encode
from datetime import date
import hashlib
import hmac
import re
import sys
from time import gmtime, strftime
from urllib import quote_plus, urlencode
from urllib2 import Request
from urlparse import urljoin, urlparse
from xml.etree.ElementTree import ElementTree

import mechanize
from BeautifulSoup import BeautifulSoup, Comment as BeautifulSoupComment

class MclsScraper(object):
	endpoint = 'http://www.rochester.lib.ny.us:2080/cgi-bin/cw_cgi'
	session = None

	def _parse_result(self, body):
		soup = BeautifulSoup(body)

		if soup.find(text=re.compile(r'No status information was found for this title\.')):
			return []
		if soup.find(text=re.compile(r'No se sabe el estado de')):
			return []
		if soup.find(text=re.compile(r'No matches were found')):
			return []

		try:
			where = soup.find(attrs={'name': 'where'}).\
				findNextSibling('table').findAll('tr')
		except:
			return []

		fields = {}
		i = 0
		for col in where[0].findAll('td'):
			value = col.find(text=True).strip()
			if value == 'Biblioteca':
				value = 'Branch'
			elif value == u'Ubicaci\xf3n':
				value = 'Location'
			elif value == 'Fecha':
				value = 'Date'
			elif value == u'Clasificaci\xf3n':
				value = 'Call Number'
			elif value == 'Estado':
				value = 'Status'
			fields[value] = i
			i += 1

		locations = []
		for location in where[1:]:
			locinfo = {}
			cells = location.findAll('td')
			for pretty, td_index in fields.iteritems():
				value = cells[td_index].find(text=True).strip()

				if pretty == 'Branch':
					value = cells[td_index].find('a')['name']
				elif pretty == 'Date':
					if value:
						month, day, year = [int(i) for i in value.split('/')]
						if year >= 0 and year <= date.today().year:
							year += 2000
						else:
							year += 1900
						value = date(year, month, day)
				elif pretty == 'Status':
					locinfo['Available'] = (value in ['Not Checked Out', 'No prestado'])

				locinfo[pretty] = value

			locations.append(locinfo)
		return locations

	def get_info(self, type, title, author):
		if not self.session:
			auth = Request('%s?5000+REDIRX' % self.endpoint)
			auth.add_header('User-Agent', 'amazon-wishlist 0.2')
			data = mechanize.urlopen(auth)

			soup = BeautifulSoup(''.join(data.fp.readlines()))
			self.session = re.sub(r'\D*(\d+)\D.*', '\\1',
				soup.find('form')['action'])

		auth = Request('%s?customSearchFinish+%s' % (self.endpoint, self.session),
			urlencode((
				('database', '720'),
				('search1', '//w'),
				('terms1', ''),
				('bool1', 'and'),
				('Search', 'Search'),
				('search2', '//n'),
				('terms2', author.encode('utf-8')),
				('bool2', 'and'),
				('search3', '//tw/'),
				('terms3', title.encode('utf-8')),
				('bool3', 'and'),
				('search4', '//sd/'),
				('terms4', ''),
				('bool4', 'and'),
				('search5', '//ed/'),
				('terms5', ''),
				('spellcheck', 'false'),
				('keywords1', type),
				('keywords2', '--none--'),
				('datelimit', '>'),
				('year1', ''),
				('year2', ''),
				('limiter_count', '3'),
				('branch', ''),
				('Available', 'no'),
			))
		)
		auth.add_header('User-Agent', 'amazon-wishlist 0.2')
		data = mechanize.urlopen(auth)

		body = ''.join(data.fp.readlines())
		soup = BeautifulSoup(body)
		result_uris = []
		for rec in soup.findAll(href=re.compile('fullRecord')):
			if rec['href'] not in result_uris:
				result_uris.append(rec['href'])
		if len(result_uris) == 0:
			return self._parse_result(body)

		locations = []
		for uri in result_uris:
			auth = Request(urljoin('%s?customSearchFinish+%s' % (self.endpoint, self.session), uri))
			auth.add_header('User-Agent', 'amazon-wishlist 0.2')
			locations.extend(self._parse_result(''.join(mechanize.urlopen(auth).fp.readlines())))
		return locations

class RitScraper(object):
	def _parse_result(body):
		soup = BeautifulSoup(body, convertEntities=BeautifulSoup.HTML_ENTITIES)

		if soup.find(text=re.compile('NO ENTRIES FOUND')):
			return []

		if soup.find(text=re.compile('Available electronically:')):
			return [{
				'LOCATION': 'ELECTRONIC',
			}]

		try:
			header = soup.find('tr', attrs={'class': 'bibItemsHeader'})
			items = soup.findAll('tr', attrs={'class': 'bibItemsEntry'})
		except:
			return []

		fields = {}
		i = 0
		for col in header.findAll('th'):
			fields[col.find(text=True).strip()] = i
			i += 1

		locations = []
		for location in items:
			locinfo = {}
			cells = location.findAll('td')
			for pretty, td_index in fields.iteritems():
				value = ''.join(cells[td_index].findAll(
					text=lambda text: not isinstance(text, BeautifulSoupComment))).strip()

				if pretty == 'LOCATION':
					value = value.replace(' FLOOR', '')
				elif pretty == 'STATUS':
					if value.split(' ')[0] == 'DUE':
						month, day, year = \
							[int(i) for i in re.sub(r'.*?(\d+-\d+-\d+).*', r'\1', value).split('-')]
						if year >= 0 and year <= date.today().year:
							year += 2000
						else:
							year += 1900
						locinfo['DUE'] = date(year, month, day)
					locinfo['AVAILABLE'] = (value == 'AVAILABLE')

				locinfo[pretty] = value

			locations.append(locinfo)
		return locations

	def get_info(type, title, author):
		auth = Request(
			'http://albert.rit.edu/search/X?t:(%s)+and+a:(%s)&searchscope=3&SORT=A' % \
			(quote_plus(title), quote_plus(author)))
		auth.add_header('User-Agent', 'amazon-wishlist 0.2')
		data = mechanize.urlopen(auth)

		body = ''.join(data.fp.readlines())
		soup = BeautifulSoup(body)

		result_uris = []
		for rec in soup.findAll(attrs={'class': 'briefCitRow'}):
			link = rec.find('a', {'href': re.compile('SUBKEY')})
			if link['href'] not in result_uris:
				result_uris.append(link['href'])
		if len(result_uris) == 0:
			return _parse_result(body)

		locations = []
		for uri in result_uris:
			item = Request(urljoin(auth.get_full_url(), uri))
			item.add_header('User-Agent', 'amazon-wishlist 0.2')
			locations.extend(_parse_result(''.join(mechanize.urlopen(item).fp.readlines())))
		return locations

def ele(ns, item, name):
	if item is None:
		return None

	qname = '{%s}%s' % (ns, ('/{%s}' % (ns)).join(name.split('/')))
	return item.find('.//%s' % qname)

def elet(item, name):
	node = ele(AWS_XML_NS, item, name)
	if node is None:
		return ''
	return node.text

def loadPage(wishlist_id, num):
	global AWS_ACCESS_KEY, AWS_SECRET_KEY, AWS_XML_NS

	mcls = MclsScraper()
	rit = RitScraper()

	qs_dict = {
		'Service': 'AWSECommerceService',
		'AWSAccessKeyId': AWS_ACCESS_KEY,
		'Operation': 'ListLookup',
		'ListType': 'WishList',
		'ListId': wishlist_id,
		'ResponseGroup': 'Medium',
		'BrowseNodes': 'True',
		'ProductPage': num,
	}
	url = sign_amazon_req(AWS_SECRET_KEY, 'GET',
		'http://webservices.amazon.com/onca/xml', qs_dict)

	auth = Request(url)
	auth.add_header('User-Agent', 'amazon-wishlist 0.2')
	data = mechanize.urlopen(auth)

	items = []
	tree = ElementTree(file=data)
	for item in tree.getroot().findall('.//{%s}Item' % (AWS_XML_NS)):
		img = ele(AWS_XML_NS, item, 'MediumImage')
		attrs = ele(AWS_XML_NS, item, 'ItemAttributes')
		offers = ele(AWS_XML_NS, item, 'OfferSummary')

		this_item = {
			'title': elet(attrs, 'Title'),
			'url': elet(item, 'DetailPageURL'),
			'sort': [
				elet(attrs, 'ProductGroup'),
				elet(item, 'BrowseNodes/BrowseNode/Ancestors/BrowseNode/Ancestors/BrowseNode/Ancestors/BrowseNode/Name'),
				elet(item, 'BrowseNodes/BrowseNode/Ancestors/BrowseNode/Ancestors/BrowseNode/Name'),
			],
			'img': {
				'url': elet(img, 'URL'),
				'width': elet(img, 'Width'),
				'height': elet(img, 'Height'),
				'caption': elet(attrs, 'Title'),
			},
			'prices': {
				'list': elet(ele(AWS_XML_NS, attrs, 'ListPrice'), 'FormattedPrice'),
				'new': {
					'count': elet(offers, 'TotalNew'),
					'lowest': elet(ele(AWS_XML_NS, offers, 'LowestNewPrice'), 'FormattedPrice'),
				},
				'used': {
					'count': elet(offers, 'TotalUsed'),
					'lowest': elet(ele(AWS_XML_NS, offers, 'LowestUsedPrice'), 'FormattedPrice'),
				},
			},
		}

		author = ele(AWS_XML_NS, attrs, 'Author')
		if author is not None:
			this_item['by'] = author.text
		artist = ele(AWS_XML_NS, attrs, 'Artist')
		if artist is not None:
			this_item['by'] = artist.text

		group = elet(attrs, 'ProductGroup')
		if group == 'Book':
			type = 'qbook'
		elif group == 'Music':
			type = 'qcd'
		else:
			type = ''

		search_title = re.sub(r'[:(].*', '', this_item['title'])
		search_title = ' '.join([
			word
			for word in search_title.split(' ')
			 if word.lower() not in
				['the', 'and', 'a', 'an', 'of', 'in', 'to', 'is', 'with']
		])
		search_author = re.sub(r'\s\w{1,3}\.(\s|$)', r'\1',
			this_item.get('by', ''))

		try:
			mcls_info = mcls.get_info(type, search_title, search_author)
		except:
			print 'Unable to retrieve MCLS info for %s' % this_item['title']
			mcls_info = []

		branches = {}
		for avail in mcls_info:
			if 'Branch' not in avail:
				continue

			if avail['Branch'] not in branches:
				branches[avail['Branch']] = {
					'count': 0,
				}
				if 'Call Number' in avail:
					branches[avail['Branch']]['call'] = avail['Call Number']

			if avail['Available']:
				branches[avail['Branch']]['count'] += 1
		this_item['mcls'] = branches

		try:
			rit_info = rit.get_info(None, search_title, this_item.get('by', ''))
		except:
			print 'Unable to retrieve RIT info for %s' % this_item['title']
			rit_info = []

		if len(rit_info) > 0:
			this_item['rit'] = {}
			if rit_info[0]['LOCATION'] == 'ELECTRONIC':
				this_item['rit']['call'] = rit_info[0]['LOCATION']
			else:
				this_item['rit']['call'] = \
					'%s %s' % (rit_info[0]['LOCATION'], rit_info[0]['CALL NO.'])
				this_item['rit']['status'] = rit_info[0]['STATUS']

		items.append(this_item)

	return items

def displayItems(items):
	global BRANCHES

	for i in range(len(items)):
		item = items[i]

		if i % 2 == 0:
			print '<li class="left">'
		else:
			print '<li class="right">'

		print '<a href="%s">' % item['url']
		print '<img style="float: left" src="%s" height="%s" width="%s" alt="%s">' % (
			item['img']['url'], item['img']['height'],
			item['img']['width'], item['img']['caption']
		)
		print '</a>'

		print item['title'], '<br>'
		if 'by' in item:
			print item['by'].encode('utf-8'), '<br>'

		print 'List:', item['prices']['list'], '<br>'
		print 'New (%s): %s, Used (%s): %s<br>' % \
			(item['prices']['new']['count'], item['prices']['new']['lowest'],
			 item['prices']['used']['count'], item['prices']['used']['lowest'])

		if 'mcls' in item and len(item['mcls']) > 0:
			print 'MCLS: '
			branches = [
				'%s (%s, %s)' % (branch, avail['count'], avail['call'])
				for branch, avail in item['mcls'].iteritems()
				if branch in BRANCHES
			]
			if len(branches) > 0:
				print ', '.join(branches)
			else:
				print ', '.join([
					'%s (%s, %s)' % (branch, avail['count'], avail['call'])
					for branch, avail in item['mcls'].iteritems()
				])

		if 'rit' in item and 'call' in item['rit']:
			if 'status' in item['rit']:
				print 'RIT: %s, %s' % \
					(item['rit']['status'], item['rit']['call'])
			else:
				print 'RIT: %s' % item['rit']['call']

		print '</li>'

def sign_amazon_req(key, method, url, qs_dict):
	parsed_url = urlparse(url)
	qs = urlencode([
		(k, qs_dict[k])
		for k
		 in sorted(qs_dict.keys())
	] + [('Timestamp', strftime('%Y-%m-%dT%H:%M:%S.000Z', gmtime()))])

	to_sign = '\n'.join([method, parsed_url.netloc, parsed_url.path, qs])
	sig = quote_plus(b64encode(
		hmac.new(
			key=key,
			msg=to_sign,
			digestmod=hashlib.sha256).digest()
	))

	return '%s?%s&Signature=%s' % (url, qs, sig)

qs_dict = {
	'Service': 'AWSECommerceService',
	'AWSAccessKeyId': AWS_ACCESS_KEY,
	'Operation': 'ListLookup',
	'ListType': 'WishList',
	'ListId': WISHLIST_ID,
}
url = sign_amazon_req(AWS_SECRET_KEY, 'GET',
	'http://webservices.amazon.com/onca/xml', qs_dict)
auth = Request(url)
auth.add_header('User-Agent', 'amazon-wishlist 0.2')

tree = ElementTree(file=mechanize.urlopen(auth))
pagecount = int(elet(tree, 'TotalPages'))

items = []
for page in range(1, pagecount + 1):
	items.extend(loadPage(WISHLIST_ID, page))
items.sort(lambda a, b: cmp('/'.join(a['sort']), '/'.join(b['sort'])))

print '''
<!DOCTYPE html>
<html>
<head>
	<title>jwm's Amazon Wishlist</title>

	<style type='text/css'>
		li {
			list-style-type: none;
			width: 50%;
		}

		li.left {
			float: left;
			clear: left;
		}
		li.right {
			float: right;
			clear: right;
		}

		img {
			border: 0;
			margin-right: 0.3em;
			margin-bottom: 0.5em;
		}
	</style>
</head>

<body>
<ul>
'''

displayItems(items)

print '''
</ul>
</body>
</html>
'''
